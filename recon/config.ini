# Configuration file for database reconstruction
# Environment variables in this file will be expanded first.
#
# If you need different variables for different systems, you add the system after an @ sign.
# For example, to set HOME to be /usr/home/system on every system except 'vacant', where the HOME
# is set to be /usr/home/vacant, use:
#  [section]
#  HOME=/usr/home/system
#  HOME@vacant=/usr/home/vacant
#
# This file is designed to be read with the version of the config file reader in the dbrecon.py
# module. A more sophisticated hiearchical config file reader is in ctools.
#
# This file has been updated so that the REIDENT environment variable
# specifies which reconstruction experiment identifier is in use.

[dbrtool]
# REIDENT separator character.
#
SEP = _
SEP_TRACTS = _tracts
DAS_ENVVAR=CLUSTERID
# Whether to skip dist verification during reident register
FAST=True

#Allow CORE nodes
USE_CORE=True
# Step1 Parallelism
S1_J1 = 50

# Step2 Parallelism
S2_J1 = 30
S2_R = 10

# Step3 parallelism
S3_J1 = 1
S3_J2 = 32

# Step4 parallelism
S4_J1 = 16
S4_J2 = 32

# Step5 parallelism
S5_J1 = 1
REFRESH_PARALLELISM = 50

[run]
# Time between system scan
#
sleep_time=10

# Name of this system:
name=recon

# Max number of threads to use (unless changed)
threads=1

# These configurations for Amazon's r5.24xlarge systems
# with 96 cores and 768GB of RAM. You might think that's a lot,
# but we can easily blow it out of the water with step3 (LP files)

# Maximum number of LP files to make at once
# There are two multithreading points.
# Remember that making LP files is memory-constrained.
# This matters especially for the large urban counties in CA, NY, IL and TX.
# We should be able to be more selective about this.

# Max number of LP jobs to run at a time.  This is the big number.
# When we ran at 8/16, we routinely ran out of memory
# This is not shared memory:
max_lp = 4

# Within a county, the number of tract LP files to make at once.
# These share memory, so it can be a higher number.
lp_j2 = 8


# We trade off making LP files with running Gurobi solutions.
# These generally do not require as much memory

# Maximum number of Gurobi instances to run at once on each system.
# 0 is unlimited
max_sol = 32

# maximum number of solutions we can launch at once on each system
max_sol_launch = 20

# No more of this total
max_jobs = 64

# Maximum load where we can still schedule. If the load goes over this, new
# jobs are not created. This can be set to be the number of cores, although the load tends to go higher.
# Jobs are not killed or suspended if we run over that, we just don't start new ones.
# However, if free memory goes below scheduler.MIN_FREE_MEM_FOR_KILLER, the scheduler will kill things
max_load = 96


# Prevously we ran on systems with different memory configurations.
# max_lp@machine1.census.gov = 4
# max_lp@machine2.census.gov = 2
# max_lp@machine3.census.gov = 2



[urls]
# Where the downloader gets the ZIP files from
SF1_URL_TEMPLATE=http://www2.census.gov/census_2010/04-Summary_File_1/{state_name}/{state_abbr}2010.sf1.zip

[paths]
# Where SF1 ZIP files distributed by the US Census Bureau are stored:
SF1_DIST=$DAS_S3ROOT/2010-re/$REIDENT_NO_SEP/dist

# On nimi we put it in a different place
SF1_DIST@nimi.local:$HOME/gits/stats_2010/sf1/

# Where the output goes
ROOT=$DAS_S3ROOT/2010-re/$REIDENT_NO_SEP

# This was when I was running at home on a computer called nimi
ROOT@nimi.local:$HOME/2010_recon

[gurobi]
# Gurobi configuration
# We set threads to 32. We've learned that additional threads don't take a lot of memory
# and there is no problem raising the load to >1000 on the DAS runs when there are 96 cores.
threads: 32
customer: Census
appname: DAS
TOKENSERVER=$GRB_TOKENSERVER
PORT=$GRB_TOKENSERVER_PORT

# Just grab the mysql values from the environment variables
[mysql]
host: $MYSQL_HOST
database: $MYSQL_DATABASE
user: $MYSQL_USER
password: $MYSQL_PASSWORD

# if verbose is 1, all MySQL queries are displayed.
verbose: 1

[environment]
# variables to copy for spark
vars: MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DATABASE,DAS_S3ROOT,DAS_S3MGMT,AWS_DEFAULT_REGION,AWS_PATH,AWS_AUTO_SCALING_HOME,AWS_ELB_HOME
